{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabe578-8166-4f62-82b9-74a70029337f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install llama_index\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-embeddings-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b9f9c-6420-4ad6-9bca-08d52e28a5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8a2f50-a641-4cac-9e7e-807b39812b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrudayte.akkalad/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed83f2-d947-406a-97d8-8b4d7a1f20cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-graph-stores-nebula\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c834c-8ee9-49ce-8b84-32d5213a0d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"./Datasets/medical_dialog_dataset/en_medical_dialog.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data[0])\n",
    "\n",
    "data = data[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c547bd-f0bd-41c7-a3e8-34b7efb0644f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c6f3e1-7f80-4e92-b13e-c9541e992ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to: ./Datasets/medical_dialog_dataset/refined_data/final_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "def process_entry(entry):\n",
    "    patient_query = entry[\"Description\"] + entry[\"Patient\"]\n",
    "    doctor_response = entry[\"Doctor\"]\n",
    "    return \"<Patient>\" + patient_query + \"<Doctor>\" + doctor_response + \"\\n\\n\"\n",
    "\n",
    "# Adjust the number of processes according to your system's capabilities\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    processed_data = pool.map(process_entry, data)\n",
    "    pool.close()  # Close the pool to prevent any more tasks from being submitted\n",
    "    pool.join()   # Wait for all processes to complete\n",
    "\n",
    "refined_data = \"\".join(processed_data)\n",
    "\n",
    "file_path = \"./Datasets/medical_dialog_dataset/refined_data/final_dataset.txt\"\n",
    "with open(file_path, \"w\") as file1:\n",
    "    file1.write(refined_data)\n",
    "\n",
    "print(\"Data written to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490e28a7-2228-4653-97aa-24a1295b9a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/jupyter/6.5.4/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/hrudayte.akkalad/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fc6db9-eb2b-433f-959e-23fc19ba6f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\"\n",
    "\n",
    "You are a QA Assistant. Your goal is to answer questions as accurates as possible based onthe instructions and context provided\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d5551-1eb3-4e37-a9bf-e36dbc3828c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f425eb-ece5-4998-ad6d-09609614fcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name = \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cc6ec-7b1d-487b-a9fc-0d9f0fad946d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a3df5b-110b-48b0-a56f-8348f2b6d1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e131aa-9e0f-4ffc-8bbb-8f6e5f5048d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(\"./Datasets/medical_dialog_dataset/refined_data/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c21775-2d3f-4384-b12b-a5eb11f2e919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/29571693/ipykernel_1813448/186755162.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed0a970-18bc-47eb-b2e4-18d4b0bd0944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bfb1fb5-f890-427d-a334-88e2081dbb92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index=VectorStoreIndex.from_documents(docs, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec94b7c-a97c-463e-84ac-ee178ff2022f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"./VectorStores/new-test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "627ef3d2-32c7-49d1-8cf0-e786663772ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb7a8e9-775e-42ab-905a-c74274f4bb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Ive had a cold which started on Christmas eve but appeared to be getting better over the following week. However I now have what I think may be sinusitis - pain in the head, yellow mucus from the nose and stuffiness-squeaking from the sinuses. Will this go away on its own or should I see my GP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a5c438-1131-4421-9b95-ac401919c067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth = \"Hi, Welcome to Chat Doctor! Yes, from what you have described, it appears that you are having Sinusitis. However, you need to be examined to diagnose it finally. So I'd recommend you to visit your GP and get yourself examined. In case the diagnosis is confirmed, you'll require a dose of Antibiotics to ward off the infection. Also, your GP will advise you certain precautions which need to be followed. Hope this information helps. Feel free to ask if you have any doubt. Wishing you a speedy recovery. With warm regards,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ee521c7-ffb5-4ff9-bf45-82854d35421b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'm just an AI, I don't have personal opinions or experiences, but I can provide you with some general information and advice based on the context you've provided.\\n\\nIt's possible that your symptoms could be related to sinusitis, which is an infection or inflammation of the sinuses. If you've recently had a cold, it's possible that your sinuses are taking longer to clear up than you expected, and this could be causing your current symptoms.\\n\\nIt's always a good idea to consult with a medical professional if you're experiencing persistent or severe symptoms, especially if they're affecting your quality of life. Your GP can assess your symptoms and provide a proper diagnosis, as well as recommend appropriate treatment options.\\n\\nIn the meantime, there are some things you can try to help manage your symptoms:\\n\\n* Use saline nasal sprays or drops to help loosen and clear out mucus from your nose.\\n* Apply a warm compress to your face to help loosen up any sinus pressure or tension.\\n* Try to breathe in some steam from a hot\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f6f037-9782-4d23-9e36-8aa7a9e89357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.24 seconds, 4.12 sentences/sec\n",
      "Precision: 0.83\n",
      "Recall: 0.86\n",
      "F1-Score: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "\n",
    "P, R, F1 = score([str(response)], [ground_truth], lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"Precision: {P.mean():.2f}\")\n",
    "print(f\"Recall: {R.mean():.2f}\")\n",
    "print(f\"F1-Score: {F1.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cdaf8-1945-4128-89d8-5aea54ea7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score([generated_response], [ground_truth_response], lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"Precision: {P.mean():.2f}\")\n",
    "print(f\"Recall: {R.mean():.2f}\")\n",
    "print(f\"F1-Score: {F1.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "735cc13a-72b6-4153-b5ff-f96c2f1e9141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.28849239894886297\n",
      "Average ROUGE Score: 0.24731182795698925\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): Hi there! I'm just an AI, I don't have personal opinions or experiences, but I can provide you with some general information and advice based on the context you've provided.\n\nIt's possible that your symptoms could be related to sinusitis, which is an infection or inflammation of the sinuses. If you've recently had a cold, it's possible that your sinuses are taking longer to clear up than you expected, and this could be causing your current symptoms.\n\nIt's always a good idea to consult with a medical professional if you're experiencing persistent or severe symptoms, especially if they're affecting your quality of life. Your GP can assess your symptoms and provide a proper diagnosis, as well as recommend appropriate treatment options.\n\nIn the meantime, there are some things you can try to help manage your symptoms:\n\n* Use saline nasal sprays or drops to help loosen and clear out mucus from your nose.\n* Apply a warm compress to your face to help loosen up any sinus pressure or tension.\n* Try to breathe in some steam from a hot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_meteor_score\u001b[39m(reference, hypothesis):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meteor_score(reference, hypothesis)\n\u001b[0;32m---> 36\u001b[0m meteor_scores \u001b[38;5;241m=\u001b[39m [calculate_meteor_score(reference, hypothesis) \u001b[38;5;28;01mfor\u001b[39;00m reference, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ground_truth_responses, generated_responses)]\n\u001b[1;32m     37\u001b[0m average_meteor_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(meteor_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(meteor_scores)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage METEOR Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_meteor_score)\n",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_meteor_score\u001b[39m(reference, hypothesis):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meteor_score(reference, hypothesis)\n\u001b[0;32m---> 36\u001b[0m meteor_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_meteor_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m reference, hypothesis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ground_truth_responses, generated_responses)]\n\u001b[1;32m     37\u001b[0m average_meteor_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(meteor_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(meteor_scores)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage METEOR Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_meteor_score)\n",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m, in \u001b[0;36mcalculate_meteor_score\u001b[0;34m(reference, hypothesis)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_meteor_score\u001b[39m(reference, hypothesis):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeteor_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:397\u001b[0m, in \u001b[0;36mmeteor_score\u001b[0;34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeteor_score\u001b[39m(\n\u001b[1;32m    348\u001b[0m     references: Iterable[Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    349\u001b[0m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m     gamma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    356\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_meteor_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwordnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:398\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeteor_score\u001b[39m(\n\u001b[1;32m    348\u001b[0m     references: Iterable[Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    349\u001b[0m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m     gamma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    356\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m--> 398\u001b[0m         \u001b[43msingle_meteor_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwordnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m reference \u001b[38;5;129;01min\u001b[39;00m references\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:326\u001b[0m, in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle_meteor_score\u001b[39m(\n\u001b[1;32m    283\u001b[0m     reference: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    284\u001b[0m     hypothesis: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m     gamma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    291\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    Calculates METEOR score for single hypothesis and reference as per\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     enum_hypothesis, enum_reference \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_enums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     translation_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(enum_hypothesis)\n\u001b[1;32m    330\u001b[0m     reference_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(enum_reference)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/nltk/translate/meteor_score.py:33\u001b[0m, in \u001b[0;36m_generate_enums\u001b[0;34m(hypothesis, reference, preprocess)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mTakes in pre-tokenized inputs for hypothesis and reference and returns\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03menumerated word lists for each of them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m:return: enumerated words list\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hypothesis, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects pre-tokenized hypothesis (Iterable[str]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhypothesis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reference, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects pre-tokenized reference (Iterable[str]): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): Hi there! I'm just an AI, I don't have personal opinions or experiences, but I can provide you with some general information and advice based on the context you've provided.\n\nIt's possible that your symptoms could be related to sinusitis, which is an infection or inflammation of the sinuses. If you've recently had a cold, it's possible that your sinuses are taking longer to clear up than you expected, and this could be causing your current symptoms.\n\nIt's always a good idea to consult with a medical professional if you're experiencing persistent or severe symptoms, especially if they're affecting your quality of life. Your GP can assess your symptoms and provide a proper diagnosis, as well as recommend appropriate treatment options.\n\nIn the meantime, there are some things you can try to help manage your symptoms:\n\n* Use saline nasal sprays or drops to help loosen and clear out mucus from your nose.\n* Apply a warm compress to your face to help loosen up any sinus pressure or tension.\n* Try to breathe in some steam from a hot"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import modified_precision, brevity_penalty\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# # Ground truth responses and model-generated responses\n",
    "ground_truth_responses = [ground_truth]  # List of ground truth responses\n",
    "generated_responses = [str(response)]     # List of model-generated responses\n",
    "\n",
    "# 1. BLEU Score\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "bleu_scores = [calculate_bleu_score(reference, hypothesis) for reference, hypothesis in zip(ground_truth_responses, generated_responses)]\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(\"Average BLEU Score:\", average_bleu_score)\n",
    "\n",
    "# 2. ROUGE Score\n",
    "def calculate_rouge_score(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return (scores['rouge1'].fmeasure + scores['rougeL'].fmeasure) / 2\n",
    "\n",
    "rouge_scores = [calculate_rouge_score(reference, hypothesis) for reference, hypothesis in zip(ground_truth_responses, generated_responses)]\n",
    "average_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "print(\"Average ROUGE Score:\", average_rouge_score)\n",
    "\n",
    "# 3. METEOR Score\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    return meteor_score(reference, hypothesis)\n",
    "\n",
    "meteor_scores = [calculate_meteor_score(reference, hypothesis) for reference, hypothesis in zip(ground_truth_responses, generated_responses)]\n",
    "average_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "print(\"Average METEOR Score:\", average_meteor_score)\n",
    "\n",
    "# 4. Accuracy (for exact matches)\n",
    "exact_matches = sum(1 for ref, gen in zip(ground_truth_responses, generated_responses) if ref == gen)\n",
    "accuracy = exact_matches / len(ground_truth_responses)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a25ea621-3824-45c5-92da-63af51638e53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /home/hrudayte.akkalad/.local/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /apps/llama/2/lib/python3.8/site-packages (from rouge_score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /apps/llama/2/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/hrudayte.akkalad/.local/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/hrudayte.akkalad/.local/lib/python3.8/site-packages (from nltk->rouge_score) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hrudayte.akkalad/.local/lib/python3.8/site-packages (from nltk->rouge_score) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /home/hrudayte.akkalad/.local/lib/python3.8/site-packages (from nltk->rouge_score) (4.66.2)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=615d37bf0cf12941c4960cb019ab56bf683da1ac1abb4adf00ba80128ba6eaf4\n",
      "  Stored in directory: /home/hrudayte.akkalad/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.1.0 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b87f01d9-611b-45f5-ad5a-c62ccd555925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db117ec4-de88-43f5-833a-9aa59ef8ae87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0686c-752b-43ab-a5b6-dce2bf4d5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./VectorStores/medical_dialog_29k/\").load_data()\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"diarrhea with headache and stomach pain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26642347-f789-43e9-af9e-8816d44bbec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"diarrhea with headache and stomach pain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d86b73f-a4f9-4ab9-94e3-ebd59169972c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm just an AI, I don't have personal experiences, but I'm here to help you with your query.\n",
      "\n",
      "Based on the context information provided, it seems like you are experiencing some discomforts like diarrhea, headache, and stomach pain. I understand that it can be quite uncomfortable and concerning.\n",
      "\n",
      "Firstly, let me suggest that you should stay hydrated by drinking plenty of fluids, especially water. Dehydration can exacerbate diarrhea and other symptoms, so it's essential to replenish your body's fluids. You can also try drinking electrolyte-rich beverages like coconut water or sports drinks to help replace lost electrolytes.\n",
      "\n",
      "In terms of managing your headache, you can try over-the-counter pain relievers like paracetamol or ibuprofen. However, please ensure that you follow the recommended dosage and consult with a medical professional if the pain persists or worsens.\n",
      "\n",
      "Regarding your stomach pain, it's possible that you may have a\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15ffa3cf-c1be-4951-8e6c-e2410fb78946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'm here to help you with your query. Based on the information provided, it seems like you're experiencing some discomforts that could be related to a few different things.\\n\\nFirstly, diarrhea can be caused by a variety of factors, such as food poisoning, viral infections, or even a change in diet. If you've recently eaten something that didn't agree with you, it could be the culprit. However, if the diarrhea persists, it's always a good idea to consult with a medical professional to rule out any underlying conditions.\\n\\nRegarding the headache and stomach pain, it's possible that they could be related to the diarrhea or another underlying condition. Headaches can be caused by a variety of factors, including tension, migraines, or even sinus pressure. Stomach pain can also be caused by a variety of factors, including digestive issues, inflammation, or even a stomach ulcer.\\n\\nIn any case, I would recommend that you consult with a medical professional to get a proper diagnosis and treatment plan. They can help you\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc11cc66-15c4-438d-8943-9f1b116ea8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/embeddings/utils.py:59\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     58\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding()\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/embeddings/openai/utils.py:104\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./VectorStores/new-test/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load index\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/indices/loading.py:33\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[0;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     index_ids \u001b[38;5;241m=\u001b[39m [index_id]\n\u001b[0;32m---> 33\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mload_indices_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/indices/loading.py:78\u001b[0m, in \u001b[0;36mload_indices_from_storage\u001b[0;34m(storage_context, index_ids, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m index_struct\u001b[38;5;241m.\u001b[39mget_type()\n\u001b[1;32m     77\u001b[0m     index_cls \u001b[38;5;241m=\u001b[39m INDEX_STRUCT_TYPE_TO_INDEX_CLASS[type_]\n\u001b[0;32m---> 78\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mindex_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/indices/vector_store/base.py:71\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43membed_model_from_settings_or_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     85\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/settings.py:274\u001b[0m, in \u001b[0;36membed_model_from_settings_or_context\u001b[0;34m(settings, context)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39membed_model\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/settings.py:67\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/embeddings/utils.py:66\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using embed_model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit our documentation for more embedding options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.html#modules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./VectorStores/new-test/\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "792d634a-3803-42fb-9bf6-ec9d673b7c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "dataset_file_path = \"./Datasets/medicare_dataset/refined_data/refined_medicare_test.txt\"  # Replace with the path to your dataset file\n",
    "\n",
    "# List to store patient queries and doctor responses\n",
    "dataset = []\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(dataset_file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "lines = lines[:50]\n",
    "print(lines[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65cacd7b-2995-4309-9c86-68862c59490e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "\n",
    "for line in lines:\n",
    "    if line != \"\\n\":\n",
    "        eval_data.append((line.split(\"<Doctor>\")[0].replace(\"<Patient>\",\"\"), line.split(\"<Doctor>\")[1].replace(\"<Doctor>\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d56e8e0-b565-4828-b80f-e0abbde50f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you for reaching out to me. I'm just an AI, I don't have personal opinions or emotions, but I'm here to help you with your query.\n",
      "\n",
      "Based on the information provided, it seems that you may be experiencing sinusitis, which can be caused by a viral or bacterial infection. While it's possible for sinusitis to clear up on its own, it's important to consult with a medical professional to determine the cause and appropriate treatment.\n",
      "\n",
      "Your GP can perform a thorough examination and may recommend further tests, such as a nasal endoscopy or CT scan, to determine the cause of your symptoms. They may also prescribe antibiotics or other medications to help manage your symptoms.\n",
      "\n",
      "In the meantime, there are some things you can do to help manage your symptoms:\n",
      "\n",
      "1. Stay hydrated by drinking plenty of fluids, such as water, tea, or soup.\n",
      "2. Use a humidifier to add moisture to the air, which can help to thin out mucus and make it easier to breathe.\n",
      "3. Apply warm compress\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def generate_response(patient_query):\n",
    "    return query_engine.query(patient_query).response\n",
    "\n",
    "patient = \"Ive had a cold which started on Christmas eve but appeared to be getting better over the following week. \\\\\n",
    "           However I now have what I think may be sinusitis - pain in the head, yellow mucus from the nose and stuffiness \\\\\n",
    "           -squeaking from the sinuses. Will this go away on its own or should I see my GP?\"\n",
    "\n",
    "print(generate_response(patient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55166e1b-46af-4921-9ae0-2cd1ef255d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# Step 1: Load the index object from the .pkl file\n",
    "with open(\"./index_medical_dialog_50k.pkl\", \"rb\") as file:\n",
    "    index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7809eb9c-8504-4ee0-a516-f8ddb51b0277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreIndex' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     index_object \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the object to a dictionary\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m index_dict \u001b[38;5;241m=\u001b[39m \u001b[43mindex_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save the dictionary to a JSON file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./index.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VectorStoreIndex' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Load the object from the pickle file\n",
    "with open('./index_medical_dialog_50k.pkl', 'rb') as f:\n",
    "    index_object = pickle.load(f)\n",
    "\n",
    "# Convert the object to a dictionary\n",
    "index_dict = index_object.to_dict()\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open('./index.json', 'w') as f:\n",
    "    json.dump(index_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "185da287-21c5-46e6-bb05-f9ae9ab2f1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "_llm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/indices/base.py:399\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    392\u001b[0m     RetrieverQueryEngine,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    396\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    397\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mllm_from_settings_or_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine\u001b[38;5;241m.\u001b[39mfrom_args(\n\u001b[1;32m    403\u001b[0m     retriever,\n\u001b[1;32m    404\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    406\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/settings.py:262\u001b[0m, in \u001b[0;36mllm_from_settings_or_context\u001b[0;34m(settings, context)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get settings from either settings or context.\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mllm\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/service_context.py:338\u001b[0m, in \u001b[0;36mServiceContext.llm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLM:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_index/core/service_context_elements/llm_predictor.py:142\u001b[0m, in \u001b[0;36mLLMPredictor.llm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLM:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _llm"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74b5b6-d9cc-4660-bc0b-02a6b6808973",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"diarrhea with headache and stomach pain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
